{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CafChem Teaching - Simple RAG-based chat with Gemma\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MauricioCafiero/CafChemTeach/blob/main/notebooks/Simple_Rag-Chat__CafChem.ipynb)\n",
        "\n",
        "## This notebook allows you to:\n",
        "- Upload a document and interact with it via RAG and the Gemma LLM.\n",
        "\n",
        "## Requirements:\n",
        "- will install all needed libraries\n",
        "- Needs a GPU. L4 or higher recommended."
      ],
      "metadata": {
        "id": "fmGzqIw_FR_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set-up"
      ],
      "metadata": {
        "id": "X09zi1mv1EJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install libraries"
      ],
      "metadata": {
        "id": "tMkJoXwl1Bc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pypdf\n",
        "! pip -q install langchain_community\n",
        "!pip -q install langchain_huggingface\n",
        "!pip -q install langchain_chroma\n",
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH3wAfJL35SE",
        "outputId": "fd332a49-c008-44d6-8fc0-957d2e73058b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.5/443.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.8/786.8 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.47.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries and define functions"
      ],
      "metadata": {
        "id": "SlhOPv5f04oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\n",
        "import torch\n",
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "import gradio as gr\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "JwlJUbhd0fol"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OO1wEttA0Ocu"
      },
      "outputs": [],
      "source": [
        "chat_history = []\n",
        "\n",
        "def clear_history():\n",
        "  '''\n",
        "    Clear the chat history.\n",
        "\n",
        "      Args:\n",
        "        None\n",
        "      Returns:\n",
        "        None\n",
        "  '''\n",
        "  global chat_history\n",
        "  chat_history = []\n",
        "\n",
        "class use_model():\n",
        "  '''\n",
        "    Class to use a Gemma model to perform retrieval augmented generation (RAG).\n",
        "  '''\n",
        "  def __init__(self, device: str, model_id = \"google/gemma-3-1b-it\"):\n",
        "    '''\n",
        "    Initialize the class.\n",
        "\n",
        "      Args:\n",
        "        device: The device to use for the model.\n",
        "        model_id: The model to use.\n",
        "      Also defines:\n",
        "        store_names: A list of the names of the stores created.\n",
        "        store_flag: A flag to indicate if a store has been created.\n",
        "    '''\n",
        "    self.model_id = model_id\n",
        "    self.device = device\n",
        "    self.store_names = []\n",
        "    self.store_flag = False\n",
        "\n",
        "  def start_model_tokenizer(self):\n",
        "    '''\n",
        "      Downloads and loads the model and tokenizer.\n",
        "\n",
        "      Args:\n",
        "        None\n",
        "      Returns:\n",
        "        None\n",
        "      Also defines:\n",
        "        model: The model to use.\n",
        "        tokenizer: The tokenizer to use.\n",
        "    '''\n",
        "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(\n",
        "        self.model_id, quantization_config=quantization_config\n",
        "    ).eval()\n",
        "\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
        "\n",
        "    print(f\"Model loaded on {self.device}\")\n",
        "\n",
        "  def create_store(self, doc_path: str, store_name: str, num_docs: int):\n",
        "    '''\n",
        "      creates a vector store from a pdf document.\n",
        "\n",
        "      Args:\n",
        "        doc_path: The path to the pdf document.\n",
        "        store_name, directory_name: The name of the store.\n",
        "        num_docs, k: The number of documents to retrive for a query.\n",
        "      Returns:\n",
        "        None\n",
        "      Also defines:\n",
        "        store: The vector store.\n",
        "        embeddings: The embeddings to use.\n",
        "    '''\n",
        "    self.k = num_docs\n",
        "\n",
        "    if store_name in self.store_names:\n",
        "      print(f\"Store already exists, calling load_store()\")\n",
        "      self.load_store(store_name)\n",
        "    else:\n",
        "      self.store_names.append(store_name)\n",
        "      self.directory_name = f\"/content/{store_name}\"\n",
        "\n",
        "\n",
        "      os.mkdir(self.directory_name)\n",
        "      print(f\"Directory '{self.directory_name}' created successfully.\")\n",
        "\n",
        "      loader = PyPDFLoader(doc_path)\n",
        "      pages = loader.load()\n",
        "\n",
        "      text_splitter = RecursiveCharacterTextSplitter(\n",
        "          chunk_size=2000,\n",
        "          chunk_overlap=200,\n",
        "          length_function=len,\n",
        "          add_start_index=True,\n",
        "      )\n",
        "      texts = text_splitter.split_documents(pages)\n",
        "\n",
        "      model_name = \"BAAI/bge-base-en-v1.5\"\n",
        "      model_kwargs = {\"device\":self.device}\n",
        "      encode_kwargs = {'normalize_embeddings':True}\n",
        "      self.embeddings = HuggingFaceEmbeddings(model_name = model_name, model_kwargs = model_kwargs,\n",
        "                                        encode_kwargs = encode_kwargs)\n",
        "\n",
        "      self.store = Chroma.from_documents(documents = texts,\n",
        "                                    embedding = self.embeddings,\n",
        "                                    persist_directory = self.directory_name)\n",
        "      print(f\"Store {store_name} created\")\n",
        "      self.store_flag = True\n",
        "\n",
        "  def load_store(self, store_name):\n",
        "    '''\n",
        "      Loads a vector store from a directory.\n",
        "\n",
        "      Args:\n",
        "        store_name: The name of the store.\n",
        "      Returns:\n",
        "        None\n",
        "    '''\n",
        "    if store_name in self.store_names:\n",
        "      self.directory_name = f\"/content/{store_name}\"\n",
        "      self.store = Chroma(persist_directory = self.directory_name, embedding_function = self.embeddings)\n",
        "      print(\"Store loaded\")\n",
        "      self.store_flag = True\n",
        "    else:\n",
        "      print(\"Store not found, call create store with a document path.\")\n",
        "\n",
        "  def search_store(self, query: str):\n",
        "    '''\n",
        "      Searches the vector store for a query.\n",
        "\n",
        "      Args:\n",
        "        query: The query to search for.\n",
        "      Returns:\n",
        "        results: The k results of the search.\n",
        "    '''\n",
        "    results = self.store.similarity_search(query, k=self.k)\n",
        "    return results\n",
        "\n",
        "\n",
        "  def chat(self, raw_prompt: str):\n",
        "    '''\n",
        "      Chats with the model.\n",
        "\n",
        "      Args:\n",
        "        raw_prompt: The prompt to send to the model.\n",
        "      Returns:\n",
        "        chat_history: The chat history.\n",
        "    '''\n",
        "    global chat_history\n",
        "\n",
        "    role_text = \"You are a helpful assistant. \"\n",
        "    prompt = \"\"\n",
        "    context = \"\"\n",
        "\n",
        "    if self.store_flag:\n",
        "      relevant_docs = self.search_store(raw_prompt)\n",
        "      context += \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "      prompt += f\"RELEVANT INFORMATION: {context}\\n\\nQUERY: {raw_prompt}\"\n",
        "      role_text += \" Use the RELEVANT INFORMATION to answer the QUERY.\"\n",
        "\n",
        "      messages = [[{\n",
        "                \"role\": \"system\",\n",
        "                \"content\": [{\"type\": \"text\", \"text\": role_text},]\n",
        "            },{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [{\"type\": \"text\", \"text\": prompt},]\n",
        "            }]]\n",
        "\n",
        "    else:\n",
        "      messages = [[{\n",
        "                  \"role\": \"system\",\n",
        "                  \"content\": [{\"type\": \"text\", \"text\": role_text},]\n",
        "              },{\n",
        "                  \"role\": \"user\",\n",
        "                  \"content\": [{\"type\": \"text\", \"text\": raw_prompt},]\n",
        "              }]]\n",
        "\n",
        "    chat_history.append({\"role\": \"user\", \"content\": raw_prompt})\n",
        "\n",
        "    inputs = self.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(self.model.device) #.to(torch.bfloat16)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = self.model.generate(**inputs, max_new_tokens=1000)\n",
        "\n",
        "    outputs = self.tokenizer.batch_decode(outputs)\n",
        "\n",
        "    parts = outputs[0].split('<start_of_turn>model')\n",
        "    response = parts[1].strip('<end_of_turn>')\n",
        "\n",
        "    chat_history.append(\n",
        "              {\"role\": \"assistant\", \"content\": response}\n",
        "  )\n",
        "\n",
        "    return '', chat_history #response, context\n",
        "\n",
        "def chatbot():\n",
        "  '''\n",
        "    A simple chatbot to interact with the model. Includes a clear button to clear the chat history, and\n",
        "    a submit button to send a message to the model.\n",
        "    Displays the entire chat history in a gradio chat interface unless the chat history is cleared.\n",
        "\n",
        "      Args:\n",
        "        None\n",
        "      Returns:\n",
        "        None\n",
        "  '''\n",
        "  with gr.Blocks() as forest:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Chat with Gemma using RAG.\n",
        "        ### Enter your messages below.\n",
        "        \"\"\")\n",
        "\n",
        "\n",
        "    chatbot = gr.Chatbot(type=\"messages\")\n",
        "    msg = gr.Textbox(label=\"Type your messages here and hit enter.\")\n",
        "    chat_btn = gr.Button(value = \"Send\")\n",
        "\n",
        "    clear = gr.ClearButton([msg, chatbot])\n",
        "    clear.click(clear_history)\n",
        "\n",
        "    chat_btn.click(chat_with_rag.chat, [msg], [msg,chatbot])\n",
        "    msg.submit(chat_with_rag.chat, [msg], [msg,chatbot])\n",
        "\n",
        "  forest.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up chat with\n",
        "- Upload your document for RAG.\n",
        "- add the path to your document to the create_store function call below.\n",
        "- run these 3 cells before starting the chatbot"
      ],
      "metadata": {
        "id": "Iv_VaGoaCOor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/gemma-3-1b-it\"\n",
        "#model_name = \"google/gemma-3-4b-it\"\n",
        "\n",
        "chat_with_rag = use_model(device, model_name)"
      ],
      "metadata": {
        "id": "pCXb3Qlr2gY1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_with_rag.start_model_tokenizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0A4Mm4U2kW4",
        "outputId": "f64ae90a-0a17-46db-b6d4-9e388218a530"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_with_rag.create_store('/content/Variable_temperature_inference_Cafiero_2025_advance.pdf','anneal', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdLH2IhzFSXL",
        "outputId": "3db0c461-8807-4690-afac-0d4c67adb4ac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory '/content/anneal' created successfully.\n",
            "Store anneal created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test RAG\n",
        "- Just some cells for testing"
      ],
      "metadata": {
        "id": "LveKTBI502B_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#chat_with_rag.load_store('anneal')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok2okCqRWlrz",
        "outputId": "6835aac6-f3e0-4cc6-b7bf-e256b79ca32a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Store not found, call create store with a document path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, history = chat_with_rag.chat(\"What is the best temperature ramp to use for molecule generation and why?\")"
      ],
      "metadata": {
        "id": "_LZwP7t-2xLU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCiZgJUJ4CcE",
        "outputId": "d238ec71-037b-4ee8-f53d-d06e5de4a121"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, history = chat_with_rag.chat(\"What is a challenging token?\")"
      ],
      "metadata": {
        "id": "-XLRw1tPYgou"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buZ2XFfFzvmP",
        "outputId": "bc1d2ba3-a758-4927-b174-48c39dfaa377"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "According to the text, a challenging token is one with high Shannon entropy, meaning that there are multiple possibilities for that token at that point in the generation process that all have similar probabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_with_rag.create_store('/content/harle-cafiero-2025-benchmark-ccsd(t)-and-density-functional-theory-calculations-of-biologically-relevant-catecholic.pdf','catechols',5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Htz4XMY31RTO",
        "outputId": "ac409046-af8c-4168-b178-517e1a8b47b4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory '/content/catechols' created successfully.\n",
            "Store catechols created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, history = chat_with_rag.chat(\"What is the best DFT method for catecholic molecules?\")"
      ],
      "metadata": {
        "id": "TaHG6eYF1e7B"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(history)"
      ],
      "metadata": {
        "id": "9QSLxcFcB_hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot\n",
        "- Click the link below to open the chat full-screen.\n",
        "- You may also share the link with others. It will work as long as this notebook is running."
      ],
      "metadata": {
        "id": "PlQJ_z1E2AZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "CJ97iuVU4vBe",
        "outputId": "58c5b03c-6a4c-4683-addb-2dd9046dee8e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a992c51ccd8879ec06.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a992c51ccd8879ec06.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zn1Hlj-c7pqC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}